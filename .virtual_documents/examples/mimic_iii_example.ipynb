


## Setup
# Import path utility first to ensure CTF can be imported
import sys
from notebook_utils import add_ctf_to_path


# Add repository root to path
# This ensures we can import the CTF module from the repository without installation
add_ctf_to_path()


# Import necessary libraries
# We need pandas for data manipulation, numpy for numerical operations,
# and matplotlib/seaborn for visualization
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Import CTF components
# These are the core classes we need from the Causal Transparency Framework
from ctf.framework import CausalTransparencyFramework  # Main framework class
from ctf.causal_discovery import CausalDiscovery       # For causal structure learning
from ctf.transparency_metrics import TransparencyMetrics  # For calculating metrics


# Set random seed for reproducibility
# This ensures our results are consistent across runs
np.random.seed(42)








# Path to the processed MIMIC-III dataset
# This should be a CSV file with clinical variables and mortality outcome
data_path = "../data/mimic_processed_for_ctf.csv"

# Load the data into a DataFrame
df = pd.read_csv(data_path)
print(f"Loaded data from {data_path}: {df.shape[0]} rows, {df.shape[1]} columns")


# Explore the dataset
print("Dataset columns:")
print(df.columns.tolist())

print("\nDataset summary:")
df.describe()


# Check target distribution
print("\nTarget (mortality) distribution:")
print(df['mortality'].value_counts(normalize=True))


# Visualize target distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='mortality', data=df)
plt.title('Mortality Distribution')
plt.xlabel('Mortality')
plt.ylabel('Count')
plt.show()





# Initialize the framework
ctf = CausalTransparencyFramework(
    data_path=data_path,
    target_col="mortality",
    output_dir="../results/mimic_iii",
    random_state=42
)


# Add domain knowledge for clinical data
domain_knowledge = {
    "edges": [
        # Clinical knowledge about mortality predictors
        # Format: [source, target, weight]
        ["age", "mortality", 0.8],
        ["sofa_score", "mortality", 0.9],
        ["lactate", "mortality", 0.7],
        ["creatinine", "mortality", 0.6],
        
        # Feature relationships
        ["age", "creatinine", 0.3],
        ["sofa_score", "lactate", 0.5]
    ]
}


# Discover causal structure
G = ctf.discover_causal_structure(domain_knowledge=domain_knowledge)
print(f"Causal graph discovered with {len(G.nodes())} nodes and {len(G.edges())} edges")


# Train predictive models
models = ctf.train_models(test_size=0.2)
print(f"Trained {len(models)} models")


# Calculate transparency metrics
metrics = ctf.calculate_transparency_metrics()
print("Transparency metrics calculated")





# Generate report
report_path = ctf.generate_report()
print(f"CTF report generated at {report_path}")


# Examine Causal Influence Index (CII)
if 'cii' in metrics:
    print("Top 5 features by Causal Influence Index (CII):")
    
    for i, (feature, score) in enumerate(list(metrics['cii'].items())[:5]):
        print(f"{i+1}. {feature}: {score:.4f}")
        
    # Visualize CII
    plt.figure(figsize=(10, 6))
    
    features = list(metrics['cii'].keys())[:10]  # Top 10 features
    scores = [metrics['cii'][f] for f in features]
    
    sns.barplot(x=scores, y=features)
    plt.title('Top Features by Causal Influence Index')
    plt.xlabel('CII')
    plt.tight_layout()
    plt.show()


# Compare model performance
model_names = list(ctf.model_performance.keys())
accuracy = [ctf.model_performance[m]['accuracy'] for m in model_names]
auc = [ctf.model_performance[m]['auc'] for m in model_names]
f1 = [ctf.model_performance[m]['f1'] for m in model_names]
n_features = [ctf.model_performance[m]['n_features'] for m in model_names]


# Create DataFrame
performance_df = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracy,
    'AUC': auc,
    'F1': f1,
    'Features': n_features,
    'Type': ['Causal' if m.startswith('causal_') else 'Full' for m in model_names]
})





# Display as table
performance_df.sort_values('AUC', ascending=False)


# Compare transparency metrics across models
if 'te' in metrics and 'cs' in metrics:
    transparency_df = pd.DataFrame({
        'Model': model_names,
        'TE': [metrics['te'][m].get('te', 0) for m in model_names],
        'CS': [metrics['cs'][m].get('overall', 0) for m in model_names],
        'Type': ['Causal' if m.startswith('causal_') else 'Full' for m in model_names]
    })
    
    # Display as table
    transparency_df.sort_values('TE', ascending=False)


# Create a scatterplot of AUC vs. TE
plt.figure(figsize=(10, 6))





# Create a combined DataFrame
combined_df = pd.merge(performance_df, transparency_df, on='Model')


# Plot
sns.scatterplot(data=combined_df, x='AUC', y='TE', hue='Type_x', size='Features', 
                sizes=(100, 400), alpha=0.7)


# Create a clear figure with proper size
plt.figure(figsize=(10, 6))

# Create scatter plot with model performance vs transparency metrics
sns.scatterplot(data=combined_df, x='AUC', y='TE', hue='Type_x', style='Type_x',
                s=150, palette=['#3366cc', '#cc3366'])

# Add model name labels with offset for readability
for i, row in combined_df.iterrows():
    plt.text(row['AUC'] + 0.005, row['TE'] + 0.005, row['Model'], fontsize=10)

# Add proper formatting
plt.title('Performance vs. Transparency Tradeoff', fontsize=14)
plt.xlabel('AUC (performance)', fontsize=12)
plt.ylabel('Transparency Entropy (interpretability)', fontsize=12)

# Set reasonable axis limits
plt.xlim(0.8, 0.9)  # Adjust based on your actual data
plt.ylim(0.05, 0.3)  # Adjust based on your actual data

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()


# Extract key findings
top_cii_features = list(metrics['cii'].items())[:3]
best_causal_model = max([m for m in model_names if m.startswith('causal_')], 
                        key=lambda m: ctf.model_performance[m]['auc'])
best_full_model = max([m for m in model_names if not m.startswith('causal_')], 
                      key=lambda m: ctf.model_performance[m]['auc'])

causal_auc = ctf.model_performance[best_causal_model]['auc']
full_auc = ctf.model_performance[best_full_model]['auc']

causal_te = metrics['te'][best_causal_model]['te']
full_te = metrics['te'][best_full_model]['te']

causal_cs = metrics['cs'][best_causal_model]['overall']
full_cs = metrics['cs'][best_full_model]['overall']


# Print key findings
print("### Key Clinical Findings ###\n")

print("1. Causal Drivers of Mortality:")
for feature, cii in top_cii_features:
    print(f"   - {feature} (CII: {cii:.4f})")

print(f"\n2. Model Performance Comparison:")
print(f"   - Best causal model ({best_causal_model}): AUC = {causal_auc:.4f}")
print(f"   - Best full model ({best_full_model}): AUC = {full_auc:.4f}")
print(f"   - Performance gap: {(full_auc - causal_auc) * 100:.2f}%")

print(f"\n3. Transparency Metrics:")
print(f"   - Transparency Entropy (TE): Causal = {causal_te:.4f}, Full = {full_te:.4f}")
print(f"   - Counterfactual Stability (CS): Causal = {causal_cs:.4f}, Full = {full_cs:.4f}")

print("\n4. Clinical Implications:")
if causal_auc >= 0.95 * full_auc:
    print("   - The causal model with fewer features performs nearly as well as the full model")
    print("   - This suggests that focusing on key causal factors may be sufficient for clinical use")
else:
    print("   - The full model substantially outperforms the causal model")
    print("   - This suggests that non-causal correlations provide important predictive value")
    
if causal_te > full_te:
    print("   - The causal model offers greater transparency, making it more interpretable for clinicians")
else:
    print("   - Despite using more features, the full model offers better interpretability")
    
if causal_cs > full_cs:
    print("   - The causal model provides more stable predictions under perturbations")
    print("   - This suggests greater reliability when input data has small variations")
else:
    print("   - The full model provides more stable predictions, possibly due to redundant features")



