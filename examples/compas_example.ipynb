{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Transparency Framework - COMPAS Example\n",
    "\n",
    "This notebook demonstrates the application of the Causal Transparency Framework (CTF) to the COMPAS recidivism dataset, focusing on fairness and bias analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The CTF provides a structured approach to evaluating and enhancing model transparency through causal reasoning. In this example with the COMPAS dataset, we'll focus on:\n",
    "\n",
    "1. Discovering causal relationships in recidivism prediction\n",
    "2. Evaluating racial bias through causal lens\n",
    "3. Comparing standard and causal models for fairness\n",
    "4. Analyzing counterfactual explanations across demographic groups\n",
    "\n",
    "This allows us to understand how the CTF can help identify and mitigate algorithmic bias in criminal justice applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import CTF components\n",
    "from ctf.framework import CausalTransparencyFramework\n",
    "from ctf.causal_discovery import CausalDiscovery\n",
    "from ctf.transparency_metrics import TransparencyMetrics\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "First, we'll load the processed COMPAS dataset. This dataset contains information about defendants and recidivism prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Path to the processed COMPAS dataset\n",
    "data_path = \"../data/processed_compas_data.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Warning: {data_path} not found.\")\n",
    "    print(\"Please download the processed COMPAS dataset or update the path.\")\n",
    "    \n",
    "    # For demonstration purposes, we'll create a small synthetic dataset\n",
    "    print(\"Creating synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create synthetic data\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Create demographics features\n",
    "    age = np.random.normal(35, 10, n_samples)\n",
    "    age = np.clip(age, 18, 70)\n",
    "    \n",
    "    # Create race variables with bias\n",
    "    race_african_american = np.random.binomial(1, 0.5, n_samples)\n",
    "    race_caucasian = 1 - race_african_american\n",
    "    \n",
    "    # Create gender variables\n",
    "    gender_male = np.random.binomial(1, 0.8, n_samples)\n",
    "    gender_female = 1 - gender_male\n",
    "    \n",
    "    # Create priors count (slightly higher for African Americans due to systemic bias)\n",
    "    base_priors = np.random.poisson(2, n_samples)\n",
    "    race_effect = race_african_american * np.random.binomial(1, 0.3, n_samples)\n",
    "    priors_count = base_priors + race_effect\n",
    "    \n",
    "    # Create charge degree\n",
    "    charge_degree_f = np.random.binomial(1, 0.3, n_samples)\n",
    "    charge_degree_m = 1 - charge_degree_f\n",
    "    \n",
    "    # Create recidivism (target - influenced by age, priors, with racial bias)\n",
    "    logits = -3 + 0.7 * priors_count - 0.03 * age + 0.3 * race_african_american\n",
    "    p_recid = 1 / (1 + np.exp(-logits))\n",
    "    two_year_recid = np.random.binomial(1, p_recid)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'race_African-American': race_african_american,\n",
    "        'race_Caucasian': race_caucasian,\n",
    "        'sex_Female': gender_female,\n",
    "        'sex_Male': gender_male,\n",
    "        'priors_count': priors_count,\n",
    "        'c_charge_degree_F': charge_degree_f,\n",
    "        'c_charge_degree_M': charge_degree_m,\n",
    "        'two_year_recid': two_year_recid\n",
    "    })\n",
    "    \n",
    "    # Save synthetic data\n",
    "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "    df.to_csv(data_path, index=False)\n",
    "    print(f\"Synthetic dataset created and saved to {data_path}\")\n",
    "else:\n",
    "    # Load the real dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded COMPAS dataset with {df.shape[0]} samples and {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Explore the dataset\n",
    "print(\"Dataset columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nDataset summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check target distribution\n",
    "print(\"\\nTarget (recidivism) distribution:\")\n",
    "print(df['two_year_recid'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize target distribution by race\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a combined race column for visualization\n",
    "if 'race_African-American' in df.columns and 'race_Caucasian' in df.columns:\n",
    "    df['race'] = np.where(df['race_African-American'] == 1, 'African-American', 'Caucasian')\n",
    "    \n",
    "    sns.countplot(x='race', hue='two_year_recid', data=df)\n",
    "    plt.title('Recidivism by Race')\n",
    "    plt.xlabel('Race')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Recidivism')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate recidivism rates by race\n",
    "    recid_by_race = df.groupby('race')['two_year_recid'].mean()\n",
    "    print(\"\\nRecidivism rates by race:\")\n",
    "    print(recid_by_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Causal Transparency Framework Application\n",
    "\n",
    "Now we'll apply the Causal Transparency Framework to analyze the dataset and train models, with a focus on fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the framework\n",
    "ctf = CausalTransparencyFramework(\n",
    "    data_path=data_path,\n",
    "    target_col=\"two_year_recid\",\n",
    "    output_dir=\"../results/compas\",\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add domain knowledge for criminal justice\n",
    "domain_knowledge = {\n",
    "    \"edges\": [\n",
    "        # Domain knowledge about recidivism predictors\n",
    "        # Format: [source, target, weight]\n",
    "        [\"priors_count\", \"two_year_recid\", 0.8],\n",
    "        [\"age\", \"two_year_recid\", 0.6],\n",
    "        [\"c_charge_degree_F\", \"two_year_recid\", 0.4],\n",
    "        \n",
    "        # Social context edges - these represent systemic bias, not causal effects\n",
    "        [\"race_African-American\", \"priors_count\", 0.3],\n",
    "        \n",
    "        # Other demographic relationships\n",
    "        [\"sex_Male\", \"priors_count\", 0.2]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Discover causal structure\n",
    "G = ctf.discover_causal_structure(domain_knowledge=domain_knowledge)\n",
    "print(f\"Causal graph discovered with {len(G.nodes())} nodes and {len(G.edges())} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train predictive models\n",
    "models = ctf.train_models(test_size=0.2)\n",
    "print(f\"Trained {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate transparency metrics\n",
    "metrics = ctf.calculate_transparency_metrics()\n",
    "print(\"Transparency metrics calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate report\n",
    "report_path = ctf.generate_report()\n",
    "print(f\"CTF report generated at {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fairness Analysis\n",
    "\n",
    "Now let's analyze the fairness implications of the models by examining how they treat different racial groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split test data by race\n",
    "X_test, y_test = ctf.test_data\n",
    "\n",
    "# Add race information back to test data\n",
    "X_test_with_race = X_test.copy()\n",
    "X_test_with_race['two_year_recid'] = y_test\n",
    "\n",
    "# Split by race\n",
    "if 'race_African-American' in X_test.columns and 'race_Caucasian' in X_test.columns:\n",
    "    X_test_african_american = X_test[X_test['race_African-American'] == 1]\n",
    "    y_test_african_american = y_test[X_test['race_African-American'] == 1]\n",
    "    \n",
    "    X_test_caucasian = X_test[X_test['race_Caucasian'] == 1]\n",
    "    y_test_caucasian = y_test[X_test['race_Caucasian'] == 1]\n",
    "    \n",
    "    print(f\"Test data: {len(X_test_african_american)} African American, {len(X_test_caucasian)} Caucasian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate fairness metrics for each model\n",
    "fairness_metrics = {}\n",
    "\n",
    "for model_name, model in ctf.models.items():\n",
    "    print(f\"\\nCalculating fairness metrics for {model_name}...\")\n",
    "    \n",
    "    # Get features used by this model\n",
    "    if model_name.startswith('causal_'):\n",
    "        # Get causal parents\n",
    "        parents = list(ctf.causal_graph.predecessors(ctf.target_col))\n",
    "        features = [p for p in parents if p in X_test.columns]\n",
    "    else:\n",
    "        # Use all features\n",
    "        features = X_test.columns.tolist()\n",
    "    \n",
    "    # Calculate predictions for each group\n",
    "    if 'race_African-American' in X_test.columns and 'race_Caucasian' in X_test.columns:\n",
    "        # African American predictions\n",
    "        X_aa = X_test_african_american[features]\n",
    "        y_aa_pred = model.predict(X_aa)\n",
    "        y_aa_pred_proba = model.predict_proba(X_aa)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Caucasian predictions\n",
    "        X_c = X_test_caucasian[features]\n",
    "        y_c_pred = model.predict(X_c)\n",
    "        y_c_pred_proba = model.predict_proba(X_c)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "        \n",
    "        # Accuracy by race\n",
    "        aa_accuracy = accuracy_score(y_test_african_american, y_aa_pred)\n",
    "        c_accuracy = accuracy_score(y_test_caucasian, y_c_pred)\n",
    "        accuracy_gap = abs(aa_accuracy - c_accuracy)\n",
    "        \n",
    "        # Prediction rates by race\n",
    "        aa_pred_rate = np.mean(y_aa_pred)\n",
    "        c_pred_rate = np.mean(y_c_pred)\n",
    "        pred_rate_gap = abs(aa_pred_rate - c_pred_rate)\n",
    "        \n",
    "        # True positive rates\n",
    "        aa_tpr = np.mean(y_aa_pred[y_test_african_american == 1]) if sum(y_test_african_american) > 0 else 0\n",
    "        c_tpr = np.mean(y_c_pred[y_test_caucasian == 1]) if sum(y_test_caucasian) > 0 else 0\n",
    "        tpr_gap = abs(aa_tpr - c_tpr)\n",
    "        \n",
    "        # False positive rates\n",
    "        aa_fpr = np.mean(y_aa_pred[y_test_african_american == 0]) if sum(y_test_african_american == 0) > 0 else 0\n",
    "        c_fpr = np.mean(y_c_pred[y_test_caucasian == 0]) if sum(y_test_caucasian == 0) > 0 else 0\n",
    "        fpr_gap = abs(aa_fpr - c_fpr)\n",
    "        \n",
    "        # AUC by race\n",
    "        if y_aa_pred_proba is not None and y_c_pred_proba is not None:\n",
    "            try:\n",
    "                aa_auc = roc_auc_score(y_test_african_american, y_aa_pred_proba)\n",
    "                c_auc = roc_auc_score(y_test_caucasian, y_c_pred_proba)\n",
    "                auc_gap = abs(aa_auc - c_auc)\n",
    "            except:\n",
    "                aa_auc, c_auc, auc_gap = 0, 0, 0\n",
    "        else:\n",
    "            aa_auc, c_auc, auc_gap = 0, 0, 0\n",
    "        \n",
    "        # Store metrics\n",
    "        fairness_metrics[model_name] = {\n",
    "            'african_american_accuracy': aa_accuracy,\n",
    "            'caucasian_accuracy': c_accuracy,\n",
    "            'accuracy_gap': accuracy_gap,\n",
    "            \n",
    "            'african_american_pred_rate': aa_pred_rate,\n",
    "            'caucasian_pred_rate': c_pred_rate,\n",
    "            'pred_rate_gap': pred_rate_gap,\n",
    "            \n",
    "            'african_american_tpr': aa_tpr,\n",
    "            'caucasian_tpr': c_tpr,\n",
    "            'tpr_gap': tpr_gap,\n",
    "            \n",
    "            'african_american_fpr': aa_fpr,\n",
    "            'caucasian_fpr': c_fpr,\n",
    "            'fpr_gap': fpr_gap,\n",
    "            \n",
    "            'african_american_auc': aa_auc,\n",
    "            'caucasian_auc': c_auc,\n",
    "            'auc_gap': auc_gap\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Accuracy: African American = {aa_accuracy:.4f}, Caucasian = {c_accuracy:.4f}, Gap = {accuracy_gap:.4f}\")\n",
    "        print(f\"  Prediction Rate: African American = {aa_pred_rate:.4f}, Caucasian = {c_pred_rate:.4f}, Gap = {pred_rate_gap:.4f}\")\n",
    "        print(f\"  TPR: African American = {aa_tpr:.4f}, Caucasian = {c_tpr:.4f}, Gap = {tpr_gap:.4f}\")\n",
    "        print(f\"  FPR: African American = {aa_fpr:.4f}, Caucasian = {c_fpr:.4f}, Gap = {fpr_gap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize fairness metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for visualization\n",
    "model_names = list(fairness_metrics.keys())\n",
    "fpr_gaps = [fairness_metrics[m]['fpr_gap'] for m in model_names]\n",
    "tpr_gaps = [fairness_metrics[m]['tpr_gap'] for m in model_names]\n",
    "pred_rate_gaps = [fairness_metrics[m]['pred_rate_gap'] for m in model_names]\n",
    "accuracy_gaps = [fairness_metrics[m]['accuracy_gap'] for m in model_names]\n",
    "\n",
    "# Model types\n",
    "model_types = ['Causal' if m.startswith('causal_') else 'Full' for m in model_names]\n",
    "\n",
    "# Create DataFrame\n",
    "fairness_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'FPR Gap': fpr_gaps,\n",
    "    'TPR Gap': tpr_gaps,\n",
    "    'Prediction Rate Gap': pred_rate_gaps,\n",
    "    'Accuracy Gap': accuracy_gaps,\n",
    "    'Type': model_types\n",
    "})\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(model_names))\n",
    "\n",
    "plt.bar(index, fpr_gaps, bar_width, label='FPR Gap', alpha=0.8)\n",
    "plt.bar(index + bar_width, tpr_gaps, bar_width, label='TPR Gap', alpha=0.8)\n",
    "plt.bar(index + 2*bar_width, pred_rate_gaps, bar_width, label='Pred Rate Gap', alpha=0.8)\n",
    "plt.bar(index + 3*bar_width, accuracy_gaps, bar_width, label='Accuracy Gap', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Gap (lower is better)')\n",
    "plt.title('Fairness Metrics by Model')\n",
    "plt.xticks(index + 1.5*bar_width, model_names, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create fairness vs. performance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Combine fairness and performance data\n",
    "model_performance = pd.DataFrame({\n",
    "    'Model': list(ctf.model_performance.keys()),\n",
    "    'AUC': [ctf.model_performance[m]['auc'] for m in ctf.model_performance.keys()],\n",
    "    'Accuracy': [ctf.model_performance[m]['accuracy'] for m in ctf.model_performance.keys()]\n",
    "})\n",
    "\n",
    "fairness_summary = pd.DataFrame({\n",
    "    'Model': list(fairness_metrics.keys()),\n",
    "    'FPR Gap': [fairness_metrics[m]['fpr_gap'] for m in fairness_metrics.keys()],\n",
    "    'Type': ['Causal' if m.startswith('causal_') else 'Full' for m in fairness_metrics.keys()]\n",
    "})\n",
    "\n",
    "combined_df = pd.merge(model_performance, fairness_summary, on='Model')\n",
    "\n",
    "# Create scatter plot: AUC vs. FPR Gap\n",
    "sns.scatterplot(data=combined_df, x='AUC', y='FPR Gap', hue='Type', style='Type',\n",
    "                s=100, palette=['#3366cc', '#cc3366'])\n",
    "\n",
    "# Add labels\n",
    "for i, row in combined_df.iterrows():\n",
    "    plt.text(row['AUC'] + 0.01, row['FPR Gap'] + 0.01, row['Model'], fontsize=8)\n",
    "\n",
    "plt.title('Performance vs. Fairness Tradeoff')\n",
    "plt.xlabel('AUC (performance)')\n",
    "plt.ylabel('FPR Gap (lower is better for fairness)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Counterfactual Stability by Race\n",
    "\n",
    "Now let's analyze the Counterfactual Stability (CS) metric, which measures how stable predictions are under small perturbations, and see if it differs by race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate CS separately for each race\n",
    "cs_by_race = {}\n",
    "\n",
    "for model_name, model in ctf.models.items():\n",
    "    print(f\"\\nCalculating CS by race for {model_name}...\")\n",
    "    \n",
    "    # Get features used by this model\n",
    "    if model_name.startswith('causal_'):\n",
    "        # Get causal parents\n",
    "        parents = list(ctf.causal_graph.predecessors(ctf.target_col))\n",
    "        features = [p for p in parents if p in X_test.columns]\n",
    "    else:\n",
    "        # Use all features\n",
    "        features = X_test.columns.tolist()\n",
    "    \n",
    "    if 'race_African-American' in X_test.columns and 'race_Caucasian' in X_test.columns:\n",
    "        # Calculate CS for African American\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            # Initialize TransparencyMetrics for African American\n",
    "            tm_aa = TransparencyMetrics(\n",
    "                causal_graph=ctf.causal_graph,\n",
    "                data=df[df['race_African-American'] == 1],\n",
    "                target_col=ctf.target_col,\n",
    "                output_dir=os.path.join(ctf.output_dir, 'fairness')\n",
    "            )\n",
    "            \n",
    "            # Calculate CS\n",
    "            cs_aa = tm_aa.calculate_cs(model, X_test_african_american[features])\n",
    "            \n",
    "            # Calculate CS for Caucasian\n",
    "            tm_c = TransparencyMetrics(\n",
    "                causal_graph=ctf.causal_graph,\n",
    "                data=df[df['race_Caucasian'] == 1],\n",
    "                target_col=ctf.target_col,\n",
    "                output_dir=os.path.join(ctf.output_dir, 'fairness')\n",
    "            )\n",
    "            \n",
    "            cs_c = tm_c.calculate_cs(model, X_test_caucasian[features])\n",
    "            \n",
    "            # Store results\n",
    "            cs_by_race[model_name] = {\n",
    "                'african_american': cs_aa.get('overall', 0),\n",
    "                'caucasian': cs_c.get('overall', 0),\n",
    "                'gap': abs(cs_aa.get('overall', 0) - cs_c.get('overall', 0))\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  CS: African American = {cs_aa.get('overall', 0):.4f}, Caucasian = {cs_c.get('overall', 0):.4f}\")\n",
    "            print(f\"  Gap: {cs_by_race[model_name]['gap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize CS by race\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Prepare data\n",
    "model_names = list(cs_by_race.keys())\n",
    "cs_aa = [cs_by_race[m]['african_american'] for m in model_names]\n",
    "cs_c = [cs_by_race[m]['caucasian'] for m in model_names]\n",
    "cs_gaps = [cs_by_race[m]['gap'] for m in model_names]\n",
    "model_types = ['Causal' if m.startswith('causal_') else 'Full' for m in model_names]\n",
    "\n",
    "# Create DataFrame\n",
    "cs_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'African American CS': cs_aa,\n",
    "    'Caucasian CS': cs_c,\n",
    "    'CS Gap': cs_gaps,\n",
    "    'Type': model_types\n",
    "})\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.3\n",
    "index = np.arange(len(model_names))\n",
    "\n",
    "plt.bar(index, cs_aa, bar_width, label='African American CS', color='#3366cc')\n",
    "plt.bar(index + bar_width, cs_c, bar_width, label='Caucasian CS', color='#cc3366')\n",
    "\n",
    "# Add line for gaps\n",
    "ax2 = plt.twinx()\n",
    "ax2.plot(index + bar_width/2, cs_gaps, 'ko-', label='CS Gap')\n",
    "ax2.set_ylabel('CS Gap (lower is better)')\n",
    "ax2.set_ylim(0, max(cs_gaps) * 1.2)\n",
    "\n",
    "# Add labels and formatting\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Counterfactual Stability (higher is better)')\n",
    "plt.title('Counterfactual Stability by Race')\n",
    "plt.xticks(index + bar_width/2, model_names, rotation=45, ha='right')\n",
    "plt.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Causal Fairness Analysis\n",
    "\n",
    "Let's analyze the causal pathways that might be contributing to bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify causal paths from race to recidivism prediction\n",
    "race_features = [col for col in df.columns if 'race_' in col]\n",
    "target_col = \"two_year_recid\"\n",
    "\n",
    "causal_paths = {}\n",
    "\n",
    "for race_feature in race_features:\n",
    "    if race_feature in ctf.causal_graph.nodes() and target_col in ctf.causal_graph.nodes():\n",
    "        # Find direct path (if any)\n",
    "        direct_path = ctf.causal_graph.has_edge(race_feature, target_col)\n",
    "        \n",
    "        # Find all paths from race to target\n",
    "        try:\n",
    "            all_paths = list(nx.all_simple_paths(ctf.causal_graph, race_feature, target_col))\n",
    "        except nx.NetworkXNoPath:\n",
    "            all_paths = []\n",
    "        \n",
    "        # Store results\n",
    "        causal_paths[race_feature] = {\n",
    "            'direct_path': direct_path,\n",
    "            'all_paths': all_paths,\n",
    "            'num_paths': len(all_paths)\n",
    "        }\n",
    "\n",
    "# Print causal paths\n",
    "print(\"Causal paths from race to recidivism:\")\n",
    "for race_feature, paths in causal_paths.items():\n",
    "    print(f\"\\n{race_feature}:\")\n",
    "    print(f\"  Direct path: {'Yes' if paths['direct_path'] else 'No'}\")\n",
    "    print(f\"  Number of paths: {paths['num_paths']}\")\n",
    "    \n",
    "    if paths['all_paths']:\n",
    "        print(\"  Paths:\")\n",
    "        for i, path in enumerate(paths['all_paths']):\n",
    "            print(f\"    {i+1}. {' -> '.join(path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate feature importance by race for the best performing model\n",
    "best_model_name = max(ctf.model_performance.keys(), \n",
    "                      key=lambda m: ctf.model_performance[m]['auc'])\n",
    "best_model = ctf.models[best_model_name]\n",
    "\n",
    "# Check if model supports feature importance\n",
    "if hasattr(best_model, 'feature_importances_') or hasattr(best_model, 'coef_'):\n",
    "    # Get features used by this model\n",
    "    if best_model_name.startswith('causal_'):\n",
    "        # Get causal parents\n",
    "        parents = list(ctf.causal_graph.predecessors(ctf.target_col))\n",
    "        features = [p for p in parents if p in X_test.columns]\n",
    "    else:\n",
    "        # Use all features\n",
    "        features = X_test.columns.tolist()\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "    else:\n",
    "        importances = np.abs(best_model.coef_[0] if len(best_model.coef_.shape) > 1 else best_model.coef_)\n",
    "    \n",
    "    # Create feature importance dictionary\n",
    "    feature_importance = dict(zip(features, importances))\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = dict(sorted(feature_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    # Display feature importance\n",
    "    print(f\"Feature importance for {best_model_name}:\")\n",
    "    for feature, importance in feature_importance.items():\n",
    "        print(f\"  {feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    features = list(feature_importance.keys())\n",
    "    importances = list(feature_importance.values())\n",
    "    \n",
    "    # Add feature type coloring\n",
    "    colors = ['#cc3366' if 'race_' in f else '#3366cc' for f in features]\n",
    "    \n",
    "    plt.barh(range(len(features)), importances, color=colors)\n",
    "    plt.yticks(range(len(features)), features)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Feature Importance for {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions and Fairness Implications\n",
    "\n",
    "Based on our analysis, let's summarize the fairness implications of the CTF applied to the COMPAS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify the best causal and full models in terms of fairness\n",
    "causal_models = [m for m in fairness_metrics.keys() if m.startswith('causal_')]\n",
    "full_models = [m for m in fairness_metrics.keys() if not m.startswith('causal_')]\n",
    "\n",
    "best_causal_fair = min(causal_models, key=lambda m: fairness_metrics[m]['fpr_gap']) if causal_models else None\n",
    "best_full_fair = min(full_models, key=lambda m: fairness_metrics[m]['fpr_gap']) if full_models else None\n",
    "\n",
    "# Get performance metrics\n",
    "causal_performance = ctf.model_performance.get(best_causal_fair, {}).get('auc', 0) if best_causal_fair else 0\n",
    "full_performance = ctf.model_performance.get(best_full_fair, {}).get('auc', 0) if best_full_fair else 0\n",
    "\n",
    "# Get fairness metrics\n",
    "causal_fairness = fairness_metrics.get(best_causal_fair, {}).get('fpr_gap', 0) if best_causal_fair else 0\n",
    "full_fairness = fairness_metrics.get(best_full_fair, {}).get('fpr_gap', 0) if best_full_fair else 0\n",
    "\n",
    "# Print conclusions\n",
    "print(\"### Fairness Implications of CTF Analysis ###\\n\")\n",
    "\n",
    "# 1. Causal structure findings\n",
    "print(\"1. Causal Structure Findings:\")\n",
    "if causal_paths and any(paths['all_paths'] for paths in causal_paths.values()):\n",
    "    print(\"   - The causal analysis reveals indirect pathways from race to recidivism prediction\")\n",
    "    print(\"   - These pathways suggest potential for disparate impact even without direct discrimination\")\n",
    "else:\n",
    "    print(\"   - No significant causal pathways from race to recidivism were found\")\n",
    "    print(\"   - This suggests that racial disparities may be due to other factors\")\n",
    "\n",
    "# 2. Model comparison\n",
    "print(\"\\n2. Model Comparison:\")\n",
    "if best_causal_fair and best_full_fair:\n",
    "    print(f\"   - Best causal model for fairness: {best_causal_fair} (AUC: {causal_performance:.4f}, FPR Gap: {causal_fairness:.4f})\")\n",
    "    print(f\"   - Best full model for fairness: {best_full_fair} (AUC: {full_performance:.4f}, FPR Gap: {full_fairness:.4f})\")\n",
    "    \n",
    "    if causal_fairness < full_fairness:\n",
    "        print(\"   - The causal model achieves better fairness across racial groups\")\n",
    "        print(\"   - This suggests that focusing on causal features can mitigate bias\")\n",
    "    else:\n",
    "        print(\"   - The full model achieves better fairness despite using more features\")\n",
    "        print(\"   - This suggests that additional non-causal features may help balance predictions\")\n",
    "    \n",
    "    perf_gap = (full_performance - causal_performance) / full_performance * 100\n",
    "    print(f\"   - The performance gap between models is {perf_gap:.2f}%\")\n",
    "\n",
    "# 3. Counterfactual stability findings\n",
    "print(\"\\n3. Counterfactual Stability Findings:\")\n",
    "if cs_by_race:\n",
    "    # Find model with lowest CS gap\n",
    "    best_cs_model = min(cs_by_race.keys(), key=lambda m: cs_by_race[m]['gap'])\n",
    "    print(f\"   - Model with most consistent CS across races: {best_cs_model} (Gap: {cs_by_race[best_cs_model]['gap']:.4f})\")\n",
    "    \n",
    "    # Check if there's a consistent pattern\n",
    "    if all(cs_by_race[m]['african_american'] < cs_by_race[m]['caucasian'] for m in cs_by_race):\n",
    "        print(\"   - Predictions for African Americans are consistently less stable than for Caucasians\")\n",
    "        print(\"   - This indicates that the model's decisions are more robust for Caucasian defendants\")\n",
    "    elif all(cs_by_race[m]['african_american'] > cs_by_race[m]['caucasian'] for m in cs_by_race):\n",
    "        print(\"   - Predictions for Caucasians are consistently less stable than for African Americans\")\n",
    "        print(\"   - This indicates that the model's decisions are more robust for African American defendants\")\n",
    "    else:\n",
    "        print(\"   - The stability pattern varies across models\")\n",
    "        print(\"   - Some models are more stable for one racial group, others for the other group\")\n",
    "\n",
    "# 4. Practical recommendations\n",
    "print(\"\\n4. Practical Recommendations:\")\n",
    "if causal_paths and any(paths['all_paths'] for paths in causal_paths.values()):\n",
    "    print(\"   - Intervene on mediating variables in the causal pathways from race to recidivism\")\n",
    "    print(\"   - This could involve addressing variables like priors_count which may reflect systemic bias\")\n",
    "    \n",
    "if cs_by_race and best_cs_model:\n",
    "    if best_cs_model.startswith('causal_'):\n",
    "        print(\"   - Prioritize causal models as they provide more consistent treatment across racial groups\")\n",
    "    else:\n",
    "        print(\"   - Consider using full models but implement additional fairness constraints\")\n",
    "    \n",
    "print(\"   - Implement fairness monitoring systems that track disparities across protected groups\")\n",
    "print(\"   - Consider using counterfactual explanations to understand and justify individual decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we applied the Causal Transparency Framework to the COMPAS dataset with a focus on fairness and bias analysis. The framework provided valuable insights into:\n",
    "\n",
    "1. The causal structure underlying recidivism prediction\n",
    "2. The pathways through which racial bias may enter the prediction system\n",
    "3. The fairness implications of different modeling approaches\n",
    "4. The stability and reliability of predictions across demographic groups\n",
    "\n",
    "The CTF approach goes beyond traditional fairness metrics by examining the causal mechanisms of bias and the structural properties of the prediction system, offering a more comprehensive framework for addressing algorithmic fairness in criminal justice applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}