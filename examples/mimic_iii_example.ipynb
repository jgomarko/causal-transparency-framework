{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6982884f",
   "metadata": {},
   "source": [
    "# Causal Transparency Framework - MIMIC-III Example\n",
    "\n",
    "This notebook demonstrates the application of the Causal Transparency Framework (CTF) to the MIMIC-III clinical dataset for mortality prediction.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The CTF provides a structured approach to evaluating and enhancing model transparency through causal reasoning. In this example, we'll:\n",
    "\n",
    "1. Load and preprocess the MIMIC-III dataset\n",
    "2. Discover causal structure\n",
    "3. Train predictive models (causal and standard)\n",
    "4. Calculate transparency metrics\n",
    "5. Generate visualizations and reports\n",
    "\n",
    "This allows us to understand the tradeoffs between model performance and transparency in clinical prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from notebook_utils import add_ctf_to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b217e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repository root to path\n",
    "add_ctf_to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e31610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CTF components\n",
    "from ctf.framework import CausalTransparencyFramework\n",
    "from ctf.causal_discovery import CausalDiscovery\n",
    "from ctf.transparency_metrics import TransparencyMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f79160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797489c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the processed MIMIC-III dataset\n",
    "data_path = \"../data/mimic_processed_for_ctf.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3edc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Warning: {data_path} not found.\")\n",
    "    print(\"Please download the processed MIMIC-III dataset or update the path.\")\n",
    "    \n",
    "    # For demonstration purposes, we'll create a small synthetic dataset\n",
    "    print(\"Creating synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create synthetic data\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Create features\n",
    "    age = np.random.normal(65, 15, n_samples)\n",
    "    gender_m = np.random.binomial(1, 0.6, n_samples)\n",
    "    \n",
    "    # Create SOFA score (influenced by age)\n",
    "    sofa = 0.2 * age + np.random.normal(0, 3, n_samples)\n",
    "    sofa = np.clip(sofa, 0, 24).astype(int)\n",
    "    \n",
    "    # Create lab values (influenced by SOFA)\n",
    "    lactate = 0.3 * sofa + np.random.normal(0, 1, n_samples)\n",
    "    lactate = np.clip(lactate, 0, 15)\n",
    "    \n",
    "    creatinine = 0.1 * sofa + 0.01 * age + np.random.normal(0, 0.5, n_samples)\n",
    "    creatinine = np.clip(creatinine, 0.3, 7)\n",
    "    \n",
    "    # Create mortality (target - influenced by age, SOFA, lactate)\n",
    "    logits = -5 + 0.02 * age + 0.3 * sofa + 0.4 * lactate\n",
    "    p_mortality = 1 / (1 + np.exp(-logits))\n",
    "    mortality = np.random.binomial(1, p_mortality)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'gender_m': gender_m,\n",
    "        'sofa_score': sofa,\n",
    "        'lactate': lactate,\n",
    "        'creatinine': creatinine,\n",
    "        'heart_rate': np.random.normal(85, 20, n_samples),\n",
    "        'respiratory_rate': np.random.normal(18, 5, n_samples),\n",
    "        'wbc': np.random.normal(10, 4, n_samples),\n",
    "        'mortality': mortality\n",
    "    })\n",
    "    \n",
    "    # Save synthetic data\n",
    "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "    df.to_csv(data_path, index=False)\n",
    "    print(f\"Synthetic dataset created and saved to {data_path}\")\n",
    "else:\n",
    "    # Load the real dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded MIMIC-III dataset with {df.shape[0]} samples and {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edb7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(\"Dataset columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nDataset summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99abca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "print(\"\\nTarget (mortality) distribution:\")\n",
    "print(df['mortality'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='mortality', data=df)\n",
    "plt.title('Mortality Distribution')\n",
    "plt.xlabel('Mortality')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the framework\n",
    "ctf = CausalTransparencyFramework(\n",
    "    data_path=data_path,\n",
    "    target_col=\"mortality\",\n",
    "    output_dir=\"../results/mimic_iii\",\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add domain knowledge for clinical data\n",
    "domain_knowledge = {\n",
    "    \"edges\": [\n",
    "        # Clinical knowledge about mortality predictors\n",
    "        # Format: [source, target, weight]\n",
    "        [\"age\", \"mortality\", 0.8],\n",
    "        [\"sofa_score\", \"mortality\", 0.9],\n",
    "        [\"lactate\", \"mortality\", 0.7],\n",
    "        [\"creatinine\", \"mortality\", 0.6],\n",
    "        \n",
    "        # Feature relationships\n",
    "        [\"age\", \"creatinine\", 0.3],\n",
    "        [\"sofa_score\", \"lactate\", 0.5]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1322b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover causal structure\n",
    "G = ctf.discover_causal_structure(domain_knowledge=domain_knowledge)\n",
    "print(f\"Causal graph discovered with {len(G.nodes())} nodes and {len(G.edges())} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b654141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train predictive models\n",
    "models = ctf.train_models(test_size=0.2)\n",
    "print(f\"Trained {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transparency metrics\n",
    "metrics = ctf.calculate_transparency_metrics()\n",
    "print(\"Transparency metrics calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report\n",
    "report_path = ctf.generate_report()\n",
    "print(f\"CTF report generated at {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e32ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Causal Influence Index (CII)\n",
    "if 'cii' in metrics:\n",
    "    print(\"Top 5 features by Causal Influence Index (CII):\")\n",
    "    \n",
    "    for i, (feature, score) in enumerate(list(metrics['cii'].items())[:5]):\n",
    "        print(f\"{i+1}. {feature}: {score:.4f}\")\n",
    "        \n",
    "    # Visualize CII\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    features = list(metrics['cii'].keys())[:10]  # Top 10 features\n",
    "    scores = [metrics['cii'][f] for f in features]\n",
    "    \n",
    "    sns.barplot(x=scores, y=features)\n",
    "    plt.title('Top Features by Causal Influence Index')\n",
    "    plt.xlabel('CII')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979aa7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "model_names = list(ctf.model_performance.keys())\n",
    "accuracy = [ctf.model_performance[m]['accuracy'] for m in model_names]\n",
    "auc = [ctf.model_performance[m]['auc'] for m in model_names]\n",
    "f1 = [ctf.model_performance[m]['f1'] for m in model_names]\n",
    "n_features = [ctf.model_performance[m]['n_features'] for m in model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c985c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "performance_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy,\n",
    "    'AUC': auc,\n",
    "    'F1': f1,\n",
    "    'Features': n_features,\n",
    "    'Type': ['Causal' if m.startswith('causal_') else 'Full' for m in model_names]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as table\n",
    "performance_df.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6066fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transparency metrics across models\n",
    "if 'te' in metrics and 'cs' in metrics:\n",
    "    transparency_df = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'TE': [metrics['te'][m].get('te', 0) for m in model_names],\n",
    "        'CS': [metrics['cs'][m].get('overall', 0) for m in model_names],\n",
    "        'Type': ['Causal' if m.startswith('causal_') else 'Full' for m in model_names]\n",
    "    })\n",
    "    \n",
    "    # Display as table\n",
    "    transparency_df.sort_values('TE', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1639406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot of AUC vs. TE\n",
    "plt.figure(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined DataFrame\n",
    "combined_df = pd.merge(performance_df, transparency_df, on='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "sns.scatterplot(data=combined_df, x='AUC', y='TE', hue='Type_x', size='Features', \n",
    "                sizes=(100, 400), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "for i, row in combined_df.iterrows():\n",
    "    plt.text(row['AUC'] + 0.005, row['TE'] + 0.005, row['Model'])\n",
    "\n",
    "plt.title('Performance vs. Transparency Tradeoff')\n",
    "plt.xlabel('AUC (performance)')\n",
    "plt.ylabel('Transparency Entropy (interpretability)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key findings\n",
    "top_cii_features = list(metrics['cii'].items())[:3]\n",
    "best_causal_model = max([m for m in model_names if m.startswith('causal_')], \n",
    "                        key=lambda m: ctf.model_performance[m]['auc'])\n",
    "best_full_model = max([m for m in model_names if not m.startswith('causal_')], \n",
    "                      key=lambda m: ctf.model_performance[m]['auc'])\n",
    "\n",
    "causal_auc = ctf.model_performance[best_causal_model]['auc']\n",
    "full_auc = ctf.model_performance[best_full_model]['auc']\n",
    "\n",
    "causal_te = metrics['te'][best_causal_model]['te']\n",
    "full_te = metrics['te'][best_full_model]['te']\n",
    "\n",
    "causal_cs = metrics['cs'][best_causal_model]['overall']\n",
    "full_cs = metrics['cs'][best_full_model]['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf85652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print key findings\n",
    "print(\"### Key Clinical Findings ###\\n\")\n",
    "\n",
    "print(\"1. Causal Drivers of Mortality:\")\n",
    "for feature, cii in top_cii_features:\n",
    "    print(f\"   - {feature} (CII: {cii:.4f})\")\n",
    "\n",
    "print(f\"\\n2. Model Performance Comparison:\")\n",
    "print(f\"   - Best causal model ({best_causal_model}): AUC = {causal_auc:.4f}\")\n",
    "print(f\"   - Best full model ({best_full_model}): AUC = {full_auc:.4f}\")\n",
    "print(f\"   - Performance gap: {(full_auc - causal_auc) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\n3. Transparency Metrics:\")\n",
    "print(f\"   - Transparency Entropy (TE): Causal = {causal_te:.4f}, Full = {full_te:.4f}\")\n",
    "print(f\"   - Counterfactual Stability (CS): Causal = {causal_cs:.4f}, Full = {full_cs:.4f}\")\n",
    "\n",
    "print(\"\\n4. Clinical Implications:\")\n",
    "if causal_auc >= 0.95 * full_auc:\n",
    "    print(\"   - The causal model with fewer features performs nearly as well as the full model\")\n",
    "    print(\"   - This suggests that focusing on key causal factors may be sufficient for clinical use\")\n",
    "else:\n",
    "    print(\"   - The full model substantially outperforms the causal model\")\n",
    "    print(\"   - This suggests that non-causal correlations provide important predictive value\")\n",
    "    \n",
    "if causal_te > full_te:\n",
    "    print(\"   - The causal model offers greater transparency, making it more interpretable for clinicians\")\n",
    "else:\n",
    "    print(\"   - Despite using more features, the full model offers better interpretability\")\n",
    "    \n",
    "if causal_cs > full_cs:\n",
    "    print(\"   - The causal model provides more stable predictions under perturbations\")\n",
    "    print(\"   - This suggests greater reliability when input data has small variations\")\n",
    "else:\n",
    "    print(\"   - The full model provides more stable predictions, possibly due to redundant features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4bd34",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we applied the Causal Transparency Framework to the MIMIC-III dataset for mortality prediction. The framework provided valuable insights into:\n",
    "\n",
    "1. The causal structure underlying mortality prediction\n",
    "2. The key causal drivers of mortality risk\n",
    "3. The performance-transparency tradeoff between causal and full models\n",
    "4. The interpretability and stability of different modeling approaches\n",
    "\n",
    "These insights can guide clinicians and data scientists in developing more transparent and reliable clinical prediction models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
